---
name: Deploy Iceberg Table

"on":
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write  # Needed for OIDC
      contents: read

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::980921712180:role/WebConsole
          aws-region: ap-south-1
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3

      - name: Install necessary utilities
        run: |
          sudo apt-get update
          sudo apt-get install -y coreutils
          sudo apt-get install -y sudo

      - name: Set up Spark environment (including JARs for Iceberg)
        run: |
          # Create directory for Spark jars
          # Create directory for Spark jars using Python
          python -c "import os; os.makedirs('/tmp/spark/jars', exist_ok=True)"

          # Download JAR files for Iceberg and Hadoop AWS
          curl -L -o /tmp/spark/jars/iceberg-spark-runtime-3.3_2.12-1.4.2.jar https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.8.1/iceberg-spark-runtime-3.5_2.12-1.8.1.jar
          curl -L -o /tmp/spark/jars/hadoop-aws-3.3.2.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar
          curl -L -o /tmp/spark/jars/aws-java-sdk-bundle-1.11.1054.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1054/aws-java-sdk-bundle-1.11.1054.jar/aws-java-sdk-bundle-1.11.271.jar
      
      - name: Install Spark (and dependencies)
        run: |
          echo "Downloading Spark 3.5.5..."
          curl -L -o spark.tgz https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz
          
          echo "Extracting Spark..."
          tar -xzf spark.tgz
          sudo mv spark-3.5.5-bin-hadoop3 /usr/local/spark

          echo "Setting up Spark environment..."
          echo "SPARK_HOME=/usr/local/spark" >> $GITHUB_ENV
          echo "PATH=/usr/local/spark/bin:$PATH" >> $GITHUB_ENV
          echo "PYSPARK_PYTHON=python" >> $GITHUB_ENV
          echo "PYSPARK_DRIVER_PYTHON=python" >> $GITHUB_ENV
      
      - name: Install dependencies
        run: |
          pip install pyspark
          pip install boto3
          pip install pyyaml

      - name: Setup Python and Spark (or use Glue Job trigger here)
        run: |
          echo "ðŸ”§ Running Iceberg Table Creation Script..."
          /usr/local/spark/bin/spark-submit \
          --jars /tmp/spark/jars/iceberg-spark-runtime-3.3_2.12-1.4.2.jar,/tmp/spark/jars/hadoop-aws-3.3.2.jar,/tmp/spark/jars/aws-java-sdk-bundle-1.11.1054.jar \
          --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \
          --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
          --conf spark.sql.catalog.glue_catalog.warehouse=s3://glue-bucket-dev-prod-bucket-march2025/warehouse/ \
          --conf spark.sql.catalog.glue_catalog.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager \
          --conf spark.sql.catalog.glue_catalog.lock.table=iceberg_lock_table \
          --conf spark.sql.defaultCatalog=glue_catalog \
          src/glue_etl/bootstrap/create_iceberg_tables.py \
          iceberg_tables/customers.yml \
          s3://glue-bucket-dev-prod-bucket-march2025/warehouse/
