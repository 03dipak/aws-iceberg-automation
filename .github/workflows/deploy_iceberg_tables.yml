name: Deploy Iceberg Table

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write  # Needed for OIDC
      contents: read

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::980921712180:role/WebConsole
          aws-region: ap-south-1
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"   
      
      - name: Check Python version
        run: python --version
      
      - name: Install dependencies
        run: |
          pip install requests
          pip install pyspark==3.4.1
          pip install boto3
          pip install pyyaml

      - name: Install Java (needed for `jar` command)
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: 🧪 Check for SparkCatalog class in Iceberg JAR
        run: |
          echo "Downloading Iceberg JAR..."
          mkdir -p /opt/aws-glue-libs/jarsv1
          curl -L -o /opt/aws-glue-libs/jarsv1/iceberg-spark-runtime-3.4_2.12-1.4.0.jar \
            https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.0/iceberg-spark-runtime-3.4_2.12-1.4.0.jar

          echo "Checking if SparkCatalog class exists..."
          jar tf /opt/aws-glue-libs/jarsv1/iceberg-spark-runtime-3.4_2.12-1.4.0.jar | \
            grep "org/apache/iceberg/spark/SparkCatalog.class" || echo "❌ SparkCatalog class NOT FOUND!"

      - name: Install Spark (and dependencies)
        run: |
          echo "Downloading Spark 3.4.4..."
          curl -L -o spark.tgz https://dlcdn.apache.org/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz
          
          echo "Extracting Spark..."
          tar -xzf spark.tgz
          sudo mv spark-3.4.4-bin-hadoop3 /usr/local/spark

          echo "Setting up Spark environment..."
          echo "SPARK_HOME=/usr/local/spark" >> $GITHUB_ENV
          echo "PATH=/usr/local/spark/bin:$PATH" >> $GITHUB_ENV
          echo "PYSPARK_PYTHON=python" >> $GITHUB_ENV
          echo "PYSPARK_DRIVER_PYTHON=python" >> $GITHUB_ENV

      - name: 🧪 Verify Spark install
        run: |
          /usr/local/spark/bin/spark-submit --version

      - name: Setup Python and Spark (or use Glue Job trigger here)
        run: |
          echo "🔧 Running Iceberg Table Creation Script..."
          /usr/local/spark/bin/spark-submit src/glue_etl/bootstrap/test_iceberg.py
